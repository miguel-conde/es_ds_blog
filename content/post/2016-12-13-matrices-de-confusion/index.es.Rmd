---
title: "Matrices de confusión"
author: Miguel Conde
date: '2016-12-13'
slug: matrices-de-confusion
categories:
  - Machine Learning
tags:
  - Confusion Matrix
description: 'La plaga zombie empezó hace ya varios meses. El gobierno informa de que, en nuestro país, con una población de 47 milones de habitantes, la prevalencia de la enfermedad es “tan solo” del 1% (es decir, el 1% de la población padece esta terrible enfermedad en un momento determinado - el actual).'
thumbnail: ''
---

La plaga zombie empezó hace ya varios meses. El gobierno informa de que, en nuestro país, con una **población** de 47 millones de habitantes, la **prevalencia** de la enfermedad es "tan solo" del 1% (es decir, el 1% de la población padece esta terrible enfermedad en un momento determinado - el actual). Desgraciadamente, la enfermedad tiene un largo periodo de incubación y se desconocen aún las causas de contagio (no, no es necesario que te muerda un infectado). Pero, por fortuna, nuestros científicos del CSIC han hecho su trabajo y han logrado una prueba diagnóstica. Nos informan de que esta prueba tiene exactitud (*accuracy*) del 95,04%. Mañana vendrán los camiones del Gobierno a tu barrio para aplicar la prueba a todos los vecinos y por fin podrás saber si tú y tus seres queridos estáis o no en peligro. Con casi total certeza (*una exactitud del 95,04% es casi total certeza ¿no?* **Pues no**. Al menos, *no exactamente*) Leyendo la letra pequeña de la noticia del CSIC en Internet nos damos cuenta de que se refieren también a otros dos "valores predictivos" de la prueba diagnóstica: su **sensibilidad** es del 99.0% y su **especificidad**, del 95.0%. *¿Qué quiere decir esto?*, te preguntas.

## Una matriz de confusión

Una prueba diagnóstica como esta nos permite clasificar a los sujetos que se someten a ella en dos clases, una positiva (*está infectado*) y otra negativa (*no está infectado*). La prueba intenta predecir el estado real de cada sujeto, pero puede no acertar. En casos como este, de clasificación binaria, pueden darse 4 posibilidades:

-   El sujeto está realmente infectado y la prueba lo diagnostica correctamente. Se trata de los casos denominados como **VERDADEROS POSITIVOS**.

-   El sujeto no está infectado y la prueba también lo diagnostica correctamente. Estamos hablando de los **VERDADEROS NEGATIVOS**.

-   El sujeto está realmente infectado y la prueba lo diagnostica equivocadamente como no infectado. Se trata de los **FALSOS NEGATIVOS**. También se conocen como **errores de tipo II**

-   El sujeto no está infectado pero la prueba lo diagnostica equivocadamente como infectado. Son los **FALSOS POSITIVOS**, **errores de tipo I** o **falsas alarmas**.

Podemos expresarlo como una tabla. Con los núemros que el Gobierno ha proporcionado - población, prevalencia, *accuracy*, sensibilidad, especificidad - los números en España resultarían ser:

[![Matriz de Confusión](http://es100x100datascience.com/wp-content/uploads/2016/12/CM.png)](http://es100x100datascience.com/wp-content/uploads/2016/12/CM.png)

## Cómo interpretarla

Como vemos, el número de realmente infectados es de 470.000:

$$
\text{Realmente infectados} = \text{Poblacion} \times \text{Prevalencia}=47.000.000 \times 1\%=470.000
$$

Sin embargo ¡nuestro test diagnostica como infectados a 2.791.800 personas! Por otra parte, el número de no infectados es de 46.530.000 personas. Aquí nuestro test acierta con 44.203.500 personas. Pero hay 4.700 que, aunque realmente están infectadas, la prueba no los detecta. **Sensibilidad** y **Especificidad** miden la *VALIDEZ* de una prueba diagnóstica, es decir, *en qué grado un ensayo mide lo que se supone debe medir*. Podemos interpretarlas de la siguiente manera:

-   **Sensibilidad** (también conocida como *recall*): es la probabilidad de que, dado que un individuo realmente está infectado, la prueba lo detecte. Es decir, podemos estimar esta probabilidad como:

$$
\text{Sensibilidad} = \frac{TP}{TP+FN}= \frac{465.300}{465.300+4.700}=99.0\%
$$

-   **Especificidad**: es la probabilidad de que, dado que un individuo no está realmente infectado, la prueba llegue a la misma conclusión. La estimamos como:

$$
\text{Especificidad} = \frac{TN}{FP + TN} = \frac{44.203.500}{2.326.500 + 44.203.500}=95.0\%
$$

Por su parte, el **Valor de Predicción Positiva** (*Positive Prediction Value*, PPV) y el **Valor de Predcción Negativa** (*Negative Prediction Values*, NPV) miden la *SEGURIDAD* de la prueba diagnóstica: *con qué seguridad el test predecirá la presencia o ausencia de enfermedad*.

-   **Valor de Predicción Positiva** (*Positive Prediction Value*, PPV), también conocido como *precision*: es la probabilidad de que, si el test ha dado positivo, el individuo esté realmente enfermo. Se estima como:

$$
\text{PPV} = \frac{TP}{TP + FP} = \frac{465.300}{465.300 + 2.326.500} = 16.67\% 
$$

-   **Valor de Predicción Negativa** (*Negative Prediction Value*, NPV): es la probabilidad de que, si el test ha dado negativo, el individuo no esté realmente enfermo. Se estima como:

$$
\text{NPV} = \frac{TN}{FN + TN} = \frac{44.203.500}{4.700 + 44.203.500} = 99.99\% 
$$

La prueba parece bastante segura: si el test me da negativo, es casi seguro (al 99,99%) que no estoy enfermo; pero si me sale positivo, la probabilidad de que realmente lo esté es sólo del 16,67%. Realmente podemos decir que *nos curamos en salud* - literalmente. De manera que la **accuracy**, es decir, el porcentaje total de aciertos del test:

$$
\text{Accuracy} = \frac{TP+TN}{TP+FP+TN+FN}
$$

no es tan importante.

## Algunas reflexiones

Resulta interesante jugar con estos números para ver cómo varían con ellos la validez y la seguridad del ensayo. Y todavía más, aplicarlos a caso reales en los que, casi siempre, de lo único que nos informan es de la *accuracy*. Las matrices de confusión son inmediatamente aplicables a los resultados de los modelos de clasificación binaria. Y se trata de un concepto casi inmediatamente aplicable a clasificadores multiclase. Os animo a pensar cómo serían en estos caso, por ejemplo cuando clasificamos nuestros ejemplos entre tres posibles clases. Por último, se puede encontrar información sobre el origen de la extraña expresión *confusion matrix* en [What is the origin of the term confusion matrix?](https://www.quora.com/What-is-the-origin-of-the-term-confusion-matrix)
