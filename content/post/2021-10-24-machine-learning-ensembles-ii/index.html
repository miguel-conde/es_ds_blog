---
title: Machine Learning Ensembles II
author: Miguel Conde
date: '2017-02-27'
slug: []
categories:
  - Machine Learning
tags:
  - 'Ensemble'
  - 'Machine Learning'
description: 'Como nos propusimos en el artículo anterior, vamos a preparar un primer ensemble entrenando un random forest, un svm tipo radial y un xgbm tipo “tree* como modelos de primer nivel.'
thumbnail: ''
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Como nos propusimos en el <a href="http://es100x100datascience.com/machine-learning-ensembles-i/">artículo anterior</a>, vamos a preparar un primer <em>ensemble</em> entrenando un <code>random forest</code>, un <code>svm</code> tipo <em>radial</em> y un <code>xgbm</code> tipo <em>tree</em> como modelos de primer nivel.</p>
<p>Para construirlos vamos a aprovechar las facilidades del paquete <a href="https://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>. Por ejemplo, nos permitirá validar los modelos construidos mediante <em>cross validation</em>, es decir, usando solo el <em>train set</em> sin necesidad de disponer de un <em>data set</em> específico para validación.</p>
<p>Como modelos de 2º nivel vamos a probar con una media, una media ponderada y una votación.</p>
<p>Como estos modelos de segundo nivel no los construiremos con <code>caret</code>, necesitaremos un <em>data set</em> específico para validarlos.</p>
<p>El código lo podéis encontrar en <a href="https://github.com/miguel-conde/example_ensemble">github</a>.</p>
<p>En primer lugar, vamos a cargar los datos:</p>
<pre class="r"><code>library(C50)

library(modeldata)

data(mlc_churn)

churn &lt;- mlc_churn</code></pre>
<p>Hemos cargado un <em>train set</em> (<code>churnTrain</code>) y un <em>test set</em> (<code>churnTest</code>). El primero lo usaremos para construir y validar los modelos y el segundo será la “prueba de fuego”, es decir, datos que no habremos visto nunca durante la construcción de los modelos y que utilizaremos como datos en condiciones reales.</p>
<p>No vamos a repetir aquí la exploración de los datos que ya hemos hecho en los posts <em>Arboles de Decisión</em> <a href="http://es100x100datascience.com/arboles-de-decision-i/">I</a>, <a href="http://es100x100datascience.com/arboles-de-decision-ii/">II</a>, <a href="http://es100x100datascience.com/arboles-de-decision-iii/">III</a> y <a href="http://es100x100datascience.com/arboles-de-decision-iv/">IV</a>, sino que vamos a ir directamente a la construcción del <em>ensemble</em>.</p>
<p>Preparemos los datos y dividamos <code>churnTrain</code> en un <em>train set</em> y un <em>validation set</em>:</p>
<pre class="r"><code># Variables target y predictoras (features)
target      &lt;- &quot;churn&quot;
predictoras &lt;- names(churn)[names(churn) != target]

# Convertimos factors a integer para que no nos de problemas con svm ni xgbm
for (v in predictoras) {
  if (is.factor(churn[, v])) {
    newName &lt;- paste0(&quot;F_&quot;, v)
    names(churn)[which(names(churn) == v)] &lt;- newName
    churn[, v] &lt;-  unclass(churn[, newName])
  }
}

set.seed(127) 
train_idx &lt;- sample(nrow(churn), 0.9*nrow(churn)) 
churnTrain &lt;- churn[train_idx,] 
churnTest &lt;- churn[-train_idx,]

rm(churn)

library(caret)
set.seed(123)
train_idx   &lt;- createDataPartition(churnTrain$churn, p = 0.75, list = FALSE)
churn_train &lt;- churnTrain[ train_idx, c(target, predictoras)]
churn_valid &lt;- churnTrain[-train_idx, c(target, predictoras)]</code></pre>
<p>Preparemos ahora los controles que vamos a utilizar al construir nuestros modelos:</p>
<pre class="r"><code>trControl &lt;- trainControl(
                          # 5-fold Cross Validation
                          method = &quot;cv&quot;, 
                          number = 5,
                          # Save the predictions for the optimal tuning 
                          # parameters
                          savePredictions = &#39;final&#39;, 
                          # Class probabilities will be computed along with
                          # predicted values in each resample
                          classProbs = TRUE
                         )</code></pre>
<p>Construimos nuestros tres modelos de primer nivel:</p>
<pre class="r"><code>f &lt;- as.formula(paste0(target, &quot;~ .&quot;))

model_rf   &lt;- train(f, churn_train,
                    method     = &quot;rf&quot;,
                    trControl  = trControl,
                    tuneLength = 3)
model_svm  &lt;- train(f, churn_train,
                    method     = &quot;svmRadial&quot;,
                    trControl  = trControl,
                    tuneLength = 3)
model_xgbm &lt;- train(f, churn_train,
                    method     = &quot;xgbTree&quot;,
                    trControl  = trControl,
                    tuneLength = 3)</code></pre>
<p>Veamos la <em>performance</em> de cada uno de los 3 modelos y comparemos (nótese que las medidas de rendimiento se toman mediante <em>cross validation</em> sobre el <em>training set</em>, no necesitamos acudir al <em>validation set</em>):</p>
<pre class="r"><code>resamps &lt;- resamples(list(rf = model_rf, svm = model_svm, xgbm = model_xgbm))
summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: rf, svm, xgbm 
## Number of resamples: 5 
## 
## Accuracy 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf   0.9482249 0.9556213 0.9569733 0.9564602 0.9600000 0.9614815    0
## svm  0.9274074 0.9318519 0.9319527 0.9315757 0.9333333 0.9333333    0
## xgbm 0.9406528 0.9497041 0.9600000 0.9558597 0.9644444 0.9644970    0
## 
## Kappa 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf   0.7660206 0.7961112 0.8004094 0.8037613 0.8228811 0.8333840    0
## svm  0.6522265 0.6718383 0.6806162 0.6752822 0.6817326 0.6899972    0
## xgbm 0.7173352 0.7647589 0.8211885 0.7970728 0.8387789 0.8433026    0</code></pre>
<pre class="r"><code>bwplot(resamps)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Los tres modelos presentan una elevada <em>accuracy</em> (rf = 0.9532, svm = 0.9212, xgbm = 0.9504), aunque la <em>kappa</em> del svm es notablemente menor que la de los otros dos.</p>
<pre class="r"><code>diffs &lt;- diff(resamps)
summary(diffs)</code></pre>
<pre><code>## 
## Call:
## summary.diff.resamples(object = diffs)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## Accuracy 
##      rf        svm        xgbm      
## rf              0.0248845  0.0006005
## svm  0.0003744            -0.0242840
## xgbm 1.0000000 0.0183899            
## 
## Kappa 
##      rf        svm       xgbm     
## rf              0.128479  0.006688
## svm  0.0006129           -0.121791
## xgbm 1.0000000 0.0175165</code></pre>
<p>Además xgbm y rf dan resultados completamente correlados.</p>
<p>A partir de esto podríamos quedarnos con svm y elegir entre xgbm y rf para, a continuación, tratar de añadir más modelos poco correlados con los dos elegidos.</p>
<p>Sin embargo, vamos a darnos por satisfechos con los tres modelos para continuar con el ejemplo y construir algunos modelos de nivel 2.</p>
<p>Lo primero que necesito son las nuevas variables predictoras, esta vez de segundo nivel. Nótese que a partir de ahora tenemos que utilizar el <em>validation set</em> para validar los modelos de segundo nivel.</p>
<pre class="r"><code># Utilizamos los modelos de 1er nivel para predecir 
churn_valid$pred_rf   &lt;- predict(object = model_rf, 
                                 churn_valid[ , predictoras])
churn_valid$pred_svm  &lt;- predict(object = model_svm, 
                                 churn_valid[ , predictoras])
churn_valid$pred_xgbm &lt;- predict(object = model_xgbm, 
                                 churn_valid[ , predictoras])

# Y sus probabilidades
churn_valid$pred_rf_prob   &lt;- predict(object = model_rf,
                                      churn_valid[,predictoras],
                                      type=&#39;prob&#39;)
churn_valid$pred_svm_prob  &lt;- predict(object = model_svm,
                                      churn_valid[,predictoras],
                                      type=&#39;prob&#39;)
churn_valid$pred_xgbm_prob &lt;- predict(object = model_xgbm,
                                      churn_valid[,predictoras],
                                      type=&#39;prob&#39;)</code></pre>
<p>Empecemos con una simple media:</p>
<pre class="r"><code>## PROMEDIO
# Calculamos la media de las predictoras de primer nivel
churn_valid$pred_avg &lt;- (churn_valid$pred_rf_prob$yes +
                           churn_valid$pred_svm_prob$yes +
                           churn_valid$pred_xgbm_prob$yes) / 3

# Dividimos las clases binarias en p = 0.5
churn_valid$pred_avg &lt;- as.factor(ifelse(churn_valid$pred_avg &gt; 0.5, 
                                         &#39;yes&#39;, &#39;no&#39;))</code></pre>
<p>Ahora la media ponderada. Como el orden de los modelos de primer nivel, según su <em>Accuracy</em>, era rf y xgbm (empatados) seguidos por svm, vamos a asignarle pesos 0.25, 0.25 y 0.5:</p>
<pre class="r"><code>## MEDIA PONDERADA
# Calculamos la media ponderada de las predictoras de primer nivel
churn_valid$pred_weighted_avg &lt;- (churn_valid$pred_rf_prob$yes * 0.25) +
  (churn_valid$pred_xgbm_prob$yes * 0.25) + 
  (churn_valid$pred_svm_prob$yes * 0.5)

# Dividimos las clases binarias en p = 0.5
churn_valid$pred_weighted_avg &lt;- as.factor(ifelse(churn_valid$pred_weighted_avg &gt; 0.5, 
                                              &#39;yes&#39;, &#39;no&#39;))</code></pre>
<p>Por último, hagamos que los modelos “voten”:</p>
<pre class="r"><code>## VOTACIÓN
# La mayoría gana
predictoras2N &lt;- c(&quot;pred_rf&quot;, &quot;pred_xgbm&quot;, &quot;pred_svm&quot;)
churn_valid$pred_majority &lt;- 
  as.factor(apply(churn_valid[, predictoras2N],
                  1, 
                  function(x) {
                    if (sum(x == &quot;yes&quot;) &gt; sum(x == &quot;no&quot;))
                      return(&quot;yes&quot;)
                    else
                      return(&quot;no&quot;)
                    }))</code></pre>
<p>Comparemos resultados contra el <em>test set</em>:</p>
<pre class="r"><code>## PROMEDIO
confusionMatrix(churn_valid$churn, churn_valid$pred_avg)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no  962   6
##        yes  42 114
##                                           
##                Accuracy : 0.9573          
##                  95% CI : (0.9438, 0.9683)
##     No Information Rate : 0.8932          
##     P-Value [Acc &gt; NIR] : 5.071e-15       
##                                           
##                   Kappa : 0.8022          
##                                           
##  Mcnemar&#39;s Test P-Value : 4.376e-07       
##                                           
##             Sensitivity : 0.9582          
##             Specificity : 0.9500          
##          Pos Pred Value : 0.9938          
##          Neg Pred Value : 0.7308          
##              Prevalence : 0.8932          
##          Detection Rate : 0.8559          
##    Detection Prevalence : 0.8612          
##       Balanced Accuracy : 0.9541          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<pre class="r"><code>## MEDIA PONDERADA
confusionMatrix(churn_valid$churn, churn_valid$pred_weighted_avg)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no  958  10
##        yes  43 113
##                                           
##                Accuracy : 0.9528          
##                  95% CI : (0.9388, 0.9645)
##     No Information Rate : 0.8906          
##     P-Value [Acc &gt; NIR] : 8.075e-14       
##                                           
##                   Kappa : 0.7835          
##                                           
##  Mcnemar&#39;s Test P-Value : 1.105e-05       
##                                           
##             Sensitivity : 0.9570          
##             Specificity : 0.9187          
##          Pos Pred Value : 0.9897          
##          Neg Pred Value : 0.7244          
##              Prevalence : 0.8906          
##          Detection Rate : 0.8523          
##    Detection Prevalence : 0.8612          
##       Balanced Accuracy : 0.9379          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<pre class="r"><code>## VOTACIÓN
confusionMatrix(churn_valid$churn, churn_valid$pred_majority)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no  964   4
##        yes  42 114
##                                           
##                Accuracy : 0.9591          
##                  95% CI : (0.9458, 0.9699)
##     No Information Rate : 0.895           
##     P-Value [Acc &gt; NIR] : 2.681e-15       
##                                           
##                   Kappa : 0.8093          
##                                           
##  Mcnemar&#39;s Test P-Value : 4.888e-08       
##                                           
##             Sensitivity : 0.9583          
##             Specificity : 0.9661          
##          Pos Pred Value : 0.9959          
##          Neg Pred Value : 0.7308          
##              Prevalence : 0.8950          
##          Detection Rate : 0.8577          
##    Detection Prevalence : 0.8612          
##       Balanced Accuracy : 0.9622          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>Como se ve, los modelos de segundo nivel media y votación dan resultados ligeramente mejores que los de primer nivel. Podríamos elegir cualquiera de los dos.</p>
<p>Supongamos que elegimos el modelo de votación. ¿Qué nos quedaría por hacer ahora? Pues construir el modelo final. Para ello, construiriamos los modelos definitivos de primer nivel utilizando esta vez <strong>todos</strong> los datos de entrenamiento (es decir, <code>churnTrain</code> completo) y los parámetros que optimizados por <code>caret</code>.</p>
<pre class="r"><code># Parámetros a utilizar
model_rf$bestTune</code></pre>
<pre><code>##   mtry
## 2   10</code></pre>
<pre class="r"><code>model_svm$bestTune</code></pre>
<pre><code>##        sigma C
## 3 0.03400684 1</code></pre>
<pre class="r"><code>model_xgbm$bestTune</code></pre>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 40      50         3 0.3     0              0.6                1      0.75</code></pre>
<pre class="r"><code>trControl &lt;- trainControl(
                          method = &quot;none&quot;, 
                          # Class probabilities will be computed along with
                          # predicted values in each resample
                          classProbs = TRUE
                         ) 

best_model_rf   &lt;- train(f, churnTrain,
                         method     = &quot;rf&quot;,
                         trControl  = trControl,
                         tuneGrid   = model_rf$bestTune)
best_model_svm  &lt;- train(f, churnTrain,
                         method     = &quot;svmRadial&quot;,
                         trControl  = trControl,
                         tuneGrid   = model_svm$bestTune)
best_model_xgbm &lt;- train(f, churnTrain,
                         method     = &quot;xgbTree&quot;,
                         trControl  = trControl,
                         tuneGrid   = model_xgbm$bestTune)</code></pre>
<p>Y ahora predeciriamos el <em>test set</em> con nuestro modelo de votación:</p>
<pre class="r"><code>churn_test &lt;- churnTest

# Utilizamos los modelos de 1er nivel para predecir 
churn_test$pred_rf   &lt;- predict(object = model_rf, 
                                churn_test[ , predictoras])
churn_test$pred_svm  &lt;- predict(object = model_svm, 
                                churn_test[ , predictoras])
churn_test$pred_xgbm &lt;- predict(object = model_xgbm,
                                churn_test[ , predictoras])

# Y sus probabilidades
churn_test$pred_rf_prob   &lt;- predict(object = model_rf,
                                     churn_test[,predictoras],
                                     type=&#39;prob&#39;)
churn_test$pred_svm_prob  &lt;- predict(object = model_svm,
                                     churn_test[,predictoras],
                                     type=&#39;prob&#39;)
churn_test$pred_xgbm_prob &lt;- predict(object = model_xgbm,
                                     churn_test[,predictoras],
                                     type=&#39;prob&#39;)

churn_test$pred_majority &lt;- 
  as.factor(apply(churn_test[, predictoras2N],
                  1, 
                  function(x) {
                    if (sum(x == &quot;yes&quot;) &gt; sum(x == &quot;no&quot;))
                      return(&quot;yes&quot;)
                    else
                      return(&quot;no&quot;)
                    }))</code></pre>
<p>Y estos son los resultados:</p>
<pre class="r"><code>## VOTACIÓN
confusionMatrix(churn_test$churn, churn_test$pred_majority)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no  416   2
##        yes  24  58
##                                           
##                Accuracy : 0.948           
##                  95% CI : (0.9247, 0.9658)
##     No Information Rate : 0.88            
##     P-Value [Acc &gt; NIR] : 1.736e-07       
##                                           
##                   Kappa : 0.7874          
##                                           
##  Mcnemar&#39;s Test P-Value : 3.814e-05       
##                                           
##             Sensitivity : 0.9455          
##             Specificity : 0.9667          
##          Pos Pred Value : 0.9952          
##          Neg Pred Value : 0.7073          
##              Prevalence : 0.8800          
##          Detection Rate : 0.8320          
##    Detection Prevalence : 0.8360          
##       Balanced Accuracy : 0.9561          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>Realmente son unos muy buenos resultados. Hasta ahora no habíamos visto estos datos de <code>churnTest</code> para nada, es la primera vez que nuestros modelos se enfrentan a ellos. Y han obtenido una <em>performance</em> comparable a la obtenida en el proceso de entrenamiento, cuando normalmente se obtiene inferior <em>performance</em> con los datos “nuevos” del <em>test set</em> que con los del <em>train set</em>, como es lógico.</p>
